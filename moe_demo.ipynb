{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoGPT + MoE Demo\n",
    "We follow the variant of MoE in [Mixtral](https://mistral.ai/news/mixtral-of-experts/).\n",
    "\n",
    "---\n",
    "\n",
    "### MoE Modules\n",
    "The following code snippet is the same as `moe_modules/moe.py`.\n",
    "\n",
    "#### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from model import LayerNorm, CausalSelfAttention\n",
    "from moe_modules.moe import Top2MLP\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self,\n",
    "                embed_dim: int,\n",
    "                n_exp: int = 8,\n",
    "                ):\n",
    "        super.__init__()\n",
    "        # Set up router net\n",
    "        self.gate_net = nn.Linear(embed_dim, n_exp, bias=False)\n",
    "        \n",
    "    def forward(self,\n",
    "                embeds: Float[Tensor, \"*batch token embed\"])->Float[Tensor, \"*batch token exp\"]:\n",
    "     \n",
    "        logits_exp = self.gate_net(embeds)\n",
    "        return logits_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    def __init__(self,\n",
    "            config,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configs\n",
    "        self.n_exp_per_token = config.n_exp_per_token\n",
    "        self.jitter_noise = config.router_jitter_noise \n",
    "        self.embed_dim = config.n_embed\n",
    "        self.n_exp = config.n_exp\n",
    "        \n",
    "        # Set up experts\n",
    "        self.exps = nn.ModuleList([Top2MLP(config) for _ in range(config.n_exp)])\n",
    "        \n",
    "        # Set up router\n",
    "        self.router = Router(embed_dim = self.embed_dim, n_exp = self.n_exp, n_exp_per_token=self.n_exp_per_token)\n",
    "        \n",
    "    def forward(self, \n",
    "                x: Float[Tensor, \"*batch token embed\"]):\n",
    "        \n",
    "        batch, n_token, hidden_dim = x.size(0), x.size(-2), x.size(-1)\n",
    "        \n",
    "        # Add random noise for capacity balance \n",
    "        if self.training and self.jitter_noise > 0:\n",
    "            x *= torch.empty_like(x).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n",
    "        \n",
    "        # routing\n",
    "        x = rearrange(x, \"batch token embed -> (batch token) embed\")\n",
    "        logits_exps = self.router(x)\n",
    "        weights_exps = F.softmax(logits_exps, dim=-1, dtype=torch.float)\n",
    "        weights_selected_exps, idx_selected_exps = torch.topk(weights_exps, self.n_exp_per_token, dim=-1) \n",
    "        weights_selected_exps = weights_selected_exps.to(x.dtype)\n",
    "        \n",
    "        # Exps forwarding \n",
    "        final_x = torch.zeros_like(x)\n",
    "        \n",
    "        mask_exps = rearrange(F.one_hot(idx_selected_exps, num_classes=self.n_exp), \"batch_token exp_per_token n_exp -> n_exp exp_per_token batch_token\")\n",
    "        \n",
    "        for id, exp in enumerate(self.exps):\n",
    "            idx_exp, top_x = torch.where(mask_exps[id])\n",
    "            \n",
    "            # Indexing token for current expert\n",
    "            x_curr = x[None, top_x].reshape(-1, hidden_dim)\n",
    "            x_curr_hidden = exp(x_curr) * weights_exps[top_x, idx_exp, None]\n",
    "            \n",
    "            final_x.index_add_(0, top_x, x_curr_hidden.to(x.dtype))\n",
    "            \n",
    "        final_x = final_x.reshape(batch, n_token, hidden_dim)\n",
    "        \n",
    "        return final_x, logits_exps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoE Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.moe = MoELayer(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Self Attention\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "\n",
    "        # FC w/ Experts\n",
    "        residual = x\n",
    "        x, logits_exps = self.moe(self.ln_2(x))\n",
    "        x = residual+ x\n",
    "        \n",
    "        return x, logits_exps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_loss_func(\n",
    "    all_router_logits: Tuple[Float[Tensor, \"*batch token exps\"]],\n",
    "    n_exp: int,\n",
    "    n_exp_per_token=int,\n",
    ") -> Tensor:\n",
    "\n",
    "    all_router_logits = torch.cat([logits_exp for logits_exp in all_router_logits], dim=0)\n",
    "    \n",
    "    # Same as routing in MoELayer\n",
    "    weights_exps = F.softmax(all_router_logits, dim=-1)\n",
    "\n",
    "    _, idx_selected_exps = torch.topk(weights_exps, n_exp_per_token, dim=-1)\n",
    "\n",
    "    mask_exps = F.one_hot(idx_selected_exps, n_exp)\n",
    " \n",
    "    n_tokens_per_exp = torch.mean(mask_exps.float(), dim=0)\n",
    "\n",
    "    prob_per_exp = torch.mean(weights_exps, dim=0)\n",
    "\n",
    "    overall_loss = torch.sum(n_tokens_per_exp * prob_per_exp.unsqueeze(0))\n",
    "    return overall_loss * n_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: compile = False\n",
      "Overriding: max_iters = 1000\n",
      "tokens per iteration will be: 16,384\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "MoE Setting:\n",
      " self.config.use_MoE=False\n",
      " self.config.alpha_balance_loss=-1\n",
      " self.config.n_exp=8\n",
      " self.config.n_exp_per_token=2\n",
      " self.config.router_jitter_noise=-1.0\n",
      "\n",
      "number of parameters: 10.65M\n",
      "/home/mcg/nanoGPT/train.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.2874, val loss 4.2823\n",
      "iter 0: loss 4.2697, time 2996.68ms, mfu -100.00%\n",
      "iter 10: loss 3.1447, time 16.33ms, mfu 22.81%\n",
      "iter 20: loss 2.7362, time 16.42ms, mfu 22.80%\n",
      "iter 30: loss 2.6184, time 16.24ms, mfu 22.81%\n",
      "iter 40: loss 2.5715, time 16.17ms, mfu 22.84%\n",
      "iter 50: loss 2.5242, time 16.24ms, mfu 22.85%\n",
      "iter 60: loss 2.5094, time 16.44ms, mfu 22.83%\n",
      "iter 70: loss 2.4960, time 16.27ms, mfu 22.84%\n",
      "iter 80: loss 2.5001, time 16.27ms, mfu 22.84%\n",
      "iter 90: loss 2.4657, time 16.50ms, mfu 22.82%\n",
      "iter 100: loss 2.4576, time 16.45ms, mfu 22.80%\n",
      "iter 110: loss 2.4471, time 16.40ms, mfu 22.79%\n",
      "iter 120: loss 2.4264, time 16.32ms, mfu 22.80%\n",
      "iter 130: loss 2.4105, time 16.24ms, mfu 22.81%\n",
      "iter 140: loss 2.3995, time 16.42ms, mfu 22.80%\n",
      "iter 150: loss 2.4031, time 16.31ms, mfu 22.80%\n",
      "iter 160: loss 2.3593, time 14.46ms, mfu 23.10%\n",
      "iter 170: loss 2.3606, time 16.26ms, mfu 23.08%\n",
      "iter 180: loss 2.3148, time 16.40ms, mfu 23.05%\n",
      "iter 190: loss 2.2482, time 16.43ms, mfu 23.01%\n",
      "iter 200: loss 2.2121, time 16.33ms, mfu 22.99%\n",
      "iter 210: loss 2.1540, time 16.33ms, mfu 22.98%\n",
      "iter 220: loss 2.1400, time 16.26ms, mfu 22.97%\n",
      "iter 230: loss 2.0710, time 16.23ms, mfu 22.97%\n",
      "iter 240: loss 2.0767, time 16.33ms, mfu 22.95%\n",
      "step 250: train loss 1.9623, val loss 2.0595\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.0363, time 3295.43ms, mfu 20.67%\n",
      "iter 260: loss 1.9761, time 16.42ms, mfu 20.87%\n",
      "iter 270: loss 1.9741, time 16.43ms, mfu 21.05%\n",
      "iter 280: loss 1.9881, time 16.28ms, mfu 21.23%\n",
      "iter 290: loss 1.9161, time 16.21ms, mfu 21.41%\n",
      "iter 300: loss 1.8987, time 16.36ms, mfu 21.55%\n",
      "iter 310: loss 1.8710, time 16.61ms, mfu 21.64%\n",
      "iter 320: loss 1.8622, time 16.29ms, mfu 21.76%\n",
      "iter 330: loss 1.8174, time 16.26ms, mfu 21.88%\n",
      "iter 340: loss 1.7779, time 16.38ms, mfu 21.96%\n",
      "iter 350: loss 1.8301, time 16.33ms, mfu 22.05%\n",
      "iter 360: loss 1.7679, time 16.26ms, mfu 22.14%\n",
      "iter 370: loss 1.7418, time 16.34ms, mfu 22.20%\n",
      "iter 380: loss 1.7232, time 16.36ms, mfu 22.26%\n",
      "iter 390: loss 1.7330, time 16.46ms, mfu 22.30%\n",
      "iter 400: loss 1.7584, time 16.47ms, mfu 22.33%\n",
      "iter 410: loss 1.6958, time 16.38ms, mfu 22.37%\n",
      "iter 420: loss 1.7062, time 16.27ms, mfu 22.43%\n",
      "iter 430: loss 1.6857, time 16.73ms, mfu 22.41%\n",
      "iter 440: loss 1.6525, time 16.41ms, mfu 22.44%\n",
      "iter 450: loss 1.6488, time 16.33ms, mfu 22.48%\n",
      "iter 460: loss 1.5947, time 16.52ms, mfu 22.49%\n",
      "iter 470: loss 1.6549, time 16.40ms, mfu 22.51%\n",
      "iter 480: loss 1.6163, time 16.37ms, mfu 22.53%\n",
      "iter 490: loss 1.6028, time 16.24ms, mfu 22.58%\n",
      "step 500: train loss 1.5261, val loss 1.7274\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.5997, time 3134.00ms, mfu 20.33%\n",
      "iter 510: loss 1.6078, time 15.77ms, mfu 20.66%\n",
      "iter 520: loss 1.5933, time 16.44ms, mfu 20.86%\n",
      "iter 530: loss 1.5625, time 16.33ms, mfu 21.06%\n",
      "iter 540: loss 1.6108, time 16.52ms, mfu 21.21%\n",
      "iter 550: loss 1.5605, time 16.47ms, mfu 21.35%\n",
      "iter 560: loss 1.5638, time 16.52ms, mfu 21.47%\n",
      "iter 570: loss 1.5632, time 16.50ms, mfu 21.58%\n",
      "iter 580: loss 1.5269, time 16.47ms, mfu 21.69%\n",
      "iter 590: loss 1.4947, time 16.57ms, mfu 21.77%\n",
      "iter 600: loss 1.5152, time 16.48ms, mfu 21.85%\n",
      "iter 610: loss 1.5420, time 16.39ms, mfu 21.94%\n",
      "iter 620: loss 1.5287, time 16.76ms, mfu 21.97%\n",
      "iter 630: loss 1.5065, time 16.22ms, mfu 22.07%\n",
      "iter 640: loss 1.4648, time 16.47ms, mfu 22.13%\n",
      "iter 650: loss 1.4991, time 16.54ms, mfu 22.17%\n",
      "iter 660: loss 1.5039, time 16.48ms, mfu 22.21%\n",
      "iter 670: loss 1.4422, time 16.49ms, mfu 22.25%\n",
      "iter 680: loss 1.5062, time 16.49ms, mfu 22.28%\n",
      "iter 690: loss 1.4563, time 16.52ms, mfu 22.31%\n",
      "iter 700: loss 1.4783, time 16.51ms, mfu 22.34%\n",
      "iter 710: loss 1.4524, time 16.45ms, mfu 22.37%\n",
      "iter 720: loss 1.4410, time 16.50ms, mfu 22.39%\n",
      "iter 730: loss 1.4177, time 16.47ms, mfu 22.41%\n",
      "iter 740: loss 1.4246, time 16.44ms, mfu 22.44%\n",
      "step 750: train loss 1.3602, val loss 1.5905\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 1.4170, time 3271.49ms, mfu 20.21%\n",
      "iter 760: loss 1.4356, time 16.55ms, mfu 20.44%\n",
      "iter 770: loss 1.4287, time 16.41ms, mfu 20.66%\n",
      "iter 780: loss 1.4202, time 16.38ms, mfu 20.87%\n",
      "iter 790: loss 1.4117, time 16.56ms, mfu 21.04%\n",
      "iter 800: loss 1.4227, time 16.52ms, mfu 21.19%\n",
      "iter 810: loss 1.4041, time 16.54ms, mfu 21.32%\n",
      "iter 820: loss 1.4070, time 16.35ms, mfu 21.47%\n",
      "iter 830: loss 1.3797, time 16.29ms, mfu 21.61%\n",
      "iter 840: loss 1.3907, time 16.42ms, mfu 21.72%\n",
      "iter 850: loss 1.3894, time 16.61ms, mfu 21.79%\n",
      "iter 860: loss 1.3915, time 16.60ms, mfu 21.85%\n",
      "iter 870: loss 1.3925, time 16.62ms, mfu 21.91%\n",
      "iter 880: loss 1.3697, time 16.61ms, mfu 21.96%\n",
      "iter 890: loss 1.3818, time 16.41ms, mfu 22.04%\n",
      "iter 900: loss 1.3660, time 16.40ms, mfu 22.11%\n",
      "iter 910: loss 1.3177, time 16.43ms, mfu 22.16%\n",
      "iter 920: loss 1.3607, time 16.49ms, mfu 22.21%\n",
      "iter 930: loss 1.3489, time 16.67ms, mfu 22.22%\n",
      "iter 940: loss 1.3441, time 16.54ms, mfu 22.25%\n",
      "iter 950: loss 1.3536, time 16.47ms, mfu 22.29%\n",
      "iter 960: loss 1.3633, time 16.47ms, mfu 22.32%\n",
      "iter 970: loss 1.3520, time 16.42ms, mfu 22.36%\n",
      "iter 980: loss 1.3526, time 16.47ms, mfu 22.39%\n",
      "iter 990: loss 1.3388, time 16.48ms, mfu 22.41%\n",
      "step 1000: train loss 1.2709, val loss 1.5226\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.3341, time 2998.87ms, mfu 20.18%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char.py --compile=False --max_iters=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoE\n",
    "Setting:\n",
    "```python\n",
    "# MoE Setting\n",
    "use_MoE = True \n",
    "alpha_balance_loss = 0.01\n",
    "n_exp = 4\n",
    "n_exp_per_token = 2\n",
    "router_jitter_noise = -1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char_moe.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "# MoE Setting\n",
      "use_MoE = True \n",
      "alpha_balance_loss = 0.01\n",
      "n_exp = 8\n",
      "n_exp_per_token = 2\n",
      "router_jitter_noise = -1.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: compile = False\n",
      "Overriding: max_iters = 1000\n",
      "Overriding: n_exp = 4\n",
      "tokens per iteration will be: 16,384\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "MoE Setting:\n",
      " self.config.use_MoE=True\n",
      " self.config.alpha_balance_loss=0.01\n",
      " self.config.n_exp=4\n",
      " self.config.n_exp_per_token=2\n",
      " self.config.router_jitter_noise=-1.0\n",
      "\n",
      "number of parameters: 31.89M\n",
      "/home/mcg/nanoGPT/train.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "num decayed parameter tensors: 68, with 31,982,976 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.1794, val loss 4.1810\n",
      "iter 0: loss 4.2267, balance loss 0.0201 6444.45ms, mfu -100.00%\n",
      "iter 10: loss 3.2802, balance loss 0.0244 41.26ms, mfu 25.25%\n",
      "iter 20: loss 2.8442, balance loss 0.0224 41.37ms, mfu 25.25%\n",
      "iter 30: loss 2.6243, balance loss 0.0204 41.06ms, mfu 25.26%\n",
      "iter 40: loss 2.5911, balance loss 0.0205 40.55ms, mfu 25.30%\n",
      "iter 50: loss 2.5287, balance loss 0.0208 41.25ms, mfu 25.30%\n",
      "iter 60: loss 2.5040, balance loss 0.0204 40.79ms, mfu 25.32%\n",
      "iter 70: loss 2.5044, balance loss 0.0208 40.93ms, mfu 25.34%\n",
      "iter 80: loss 2.4982, balance loss 0.0207 40.36ms, mfu 25.38%\n",
      "iter 90: loss 2.5029, balance loss 0.0206 42.82ms, mfu 25.28%\n",
      "iter 100: loss 2.4733, balance loss 0.0206 40.42ms, mfu 25.33%\n",
      "iter 110: loss 2.4706, balance loss 0.0205 40.23ms, mfu 25.39%\n",
      "iter 120: loss 2.4593, balance loss 0.0207 41.01ms, mfu 25.39%\n",
      "iter 130: loss 2.4452, balance loss 0.0204 40.77ms, mfu 25.41%\n",
      "iter 140: loss 2.4230, balance loss 0.0207 39.65ms, mfu 25.49%\n",
      "iter 150: loss 2.3960, balance loss 0.0209 40.54ms, mfu 25.51%\n",
      "iter 160: loss 2.4095, balance loss 0.0205 41.01ms, mfu 25.50%\n",
      "iter 170: loss 2.3385, balance loss 0.0209 41.08ms, mfu 25.49%\n",
      "iter 180: loss 2.2965, balance loss 0.0206 41.23ms, mfu 25.47%\n",
      "iter 190: loss 2.2738, balance loss 0.0207 41.74ms, mfu 25.42%\n",
      "iter 200: loss 2.2367, balance loss 0.0207 40.76ms, mfu 25.43%\n",
      "iter 210: loss 2.1812, balance loss 0.0207 41.51ms, mfu 25.40%\n",
      "iter 220: loss 2.1473, balance loss 0.0208 39.10ms, mfu 25.52%\n",
      "iter 230: loss 2.0756, balance loss 0.0206 40.51ms, mfu 25.54%\n",
      "iter 240: loss 2.0416, balance loss 0.0207 40.25ms, mfu 25.58%\n",
      "step 250: train loss 1.9376, val loss 2.0481\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.0092, balance loss 0.0208 7213.48ms, mfu 23.03%\n",
      "iter 260: loss 1.9837, balance loss 0.0207 39.49ms, mfu 23.37%\n",
      "iter 270: loss 1.9862, balance loss 0.0208 42.94ms, mfu 23.46%\n",
      "iter 280: loss 1.9622, balance loss 0.0207 40.53ms, mfu 23.68%\n",
      "iter 290: loss 1.8771, balance loss 0.0207 40.75ms, mfu 23.87%\n",
      "iter 300: loss 1.8972, balance loss 0.0207 39.67ms, mfu 24.11%\n",
      "iter 310: loss 1.8745, balance loss 0.0209 40.86ms, mfu 24.25%\n",
      "iter 320: loss 1.8257, balance loss 0.0209 41.25ms, mfu 24.35%\n",
      "iter 330: loss 1.8264, balance loss 0.0209 40.99ms, mfu 24.46%\n",
      "iter 340: loss 1.7900, balance loss 0.0210 40.51ms, mfu 24.58%\n",
      "iter 350: loss 1.7503, balance loss 0.0209 40.46ms, mfu 24.70%\n",
      "iter 360: loss 1.7688, balance loss 0.0208 40.95ms, mfu 24.78%\n",
      "iter 370: loss 1.7353, balance loss 0.0209 42.22ms, mfu 24.77%\n",
      "iter 380: loss 1.7238, balance loss 0.0209 40.69ms, mfu 24.85%\n",
      "iter 390: loss 1.7196, balance loss 0.0209 40.91ms, mfu 24.91%\n",
      "iter 400: loss 1.6980, balance loss 0.0212 40.86ms, mfu 24.97%\n",
      "iter 410: loss 1.6873, balance loss 0.0210 41.94ms, mfu 24.96%\n",
      "iter 420: loss 1.6786, balance loss 0.0210 40.87ms, mfu 25.01%\n",
      "iter 430: loss 1.6572, balance loss 0.0210 40.99ms, mfu 25.05%\n",
      "iter 440: loss 1.6393, balance loss 0.0210 40.98ms, mfu 25.09%\n",
      "iter 450: loss 1.6108, balance loss 0.0211 40.86ms, mfu 25.13%\n",
      "iter 460: loss 1.6264, balance loss 0.0211 40.92ms, mfu 25.16%\n",
      "iter 470: loss 1.6262, balance loss 0.0209 41.49ms, mfu 25.16%\n",
      "iter 480: loss 1.6060, balance loss 0.0211 42.11ms, mfu 25.12%\n",
      "iter 490: loss 1.5957, balance loss 0.0211 41.39ms, mfu 25.12%\n",
      "step 500: train loss 1.4869, val loss 1.6861\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.5997, balance loss 0.0213 6911.92ms, mfu 22.63%\n",
      "iter 510: loss 1.5734, balance loss 0.0212 40.54ms, mfu 22.93%\n",
      "iter 520: loss 1.5853, balance loss 0.0213 40.84ms, mfu 23.19%\n",
      "iter 530: loss 1.5289, balance loss 0.0214 41.08ms, mfu 23.41%\n",
      "iter 540: loss 1.5102, balance loss 0.0213 40.94ms, mfu 23.61%\n",
      "iter 550: loss 1.5524, balance loss 0.0212 40.42ms, mfu 23.83%\n",
      "iter 560: loss 1.5896, balance loss 0.0215 41.32ms, mfu 23.97%\n",
      "iter 570: loss 1.5225, balance loss 0.0214 40.82ms, mfu 24.12%\n",
      "iter 580: loss 1.5119, balance loss 0.0215 39.40ms, mfu 24.36%\n",
      "iter 590: loss 1.4962, balance loss 0.0214 40.26ms, mfu 24.51%\n",
      "iter 600: loss 1.5275, balance loss 0.0214 40.02ms, mfu 24.66%\n",
      "iter 610: loss 1.5199, balance loss 0.0215 40.86ms, mfu 24.74%\n",
      "iter 620: loss 1.4789, balance loss 0.0215 41.29ms, mfu 24.79%\n",
      "iter 630: loss 1.4709, balance loss 0.0215 40.42ms, mfu 24.89%\n",
      "iter 640: loss 1.4779, balance loss 0.0215 40.89ms, mfu 24.95%\n",
      "iter 650: loss 1.4548, balance loss 0.0215 41.07ms, mfu 24.99%\n",
      "iter 660: loss 1.4761, balance loss 0.0215 40.09ms, mfu 25.09%\n",
      "iter 670: loss 1.4686, balance loss 0.0212 40.36ms, mfu 25.16%\n",
      "iter 680: loss 1.4593, balance loss 0.0215 38.89ms, mfu 25.33%\n",
      "iter 690: loss 1.4225, balance loss 0.0212 40.99ms, mfu 25.34%\n",
      "iter 700: loss 1.4561, balance loss 0.0212 42.77ms, mfu 25.24%\n",
      "iter 710: loss 1.4532, balance loss 0.0216 41.01ms, mfu 25.26%\n",
      "iter 720: loss 1.4408, balance loss 0.0214 40.77ms, mfu 25.29%\n",
      "iter 730: loss 1.4135, balance loss 0.0214 41.67ms, mfu 25.26%\n",
      "iter 740: loss 1.4424, balance loss 0.0213 40.78ms, mfu 25.29%\n",
      "step 750: train loss 1.3290, val loss 1.5662\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 1.4161, balance loss 0.0214 6722.98ms, mfu 22.77%\n",
      "iter 760: loss 1.4214, balance loss 0.0212 40.57ms, mfu 23.06%\n",
      "iter 770: loss 1.3917, balance loss 0.0216 41.09ms, mfu 23.29%\n",
      "iter 780: loss 1.3980, balance loss 0.0213 40.20ms, mfu 23.56%\n",
      "iter 790: loss 1.3998, balance loss 0.0213 40.36ms, mfu 23.78%\n",
      "iter 800: loss 1.3711, balance loss 0.0216 40.91ms, mfu 23.95%\n",
      "iter 810: loss 1.3645, balance loss 0.0216 40.41ms, mfu 24.13%\n",
      "iter 820: loss 1.3681, balance loss 0.0215 40.37ms, mfu 24.30%\n",
      "iter 830: loss 1.3742, balance loss 0.0214 40.25ms, mfu 24.46%\n",
      "iter 840: loss 1.3932, balance loss 0.0216 40.50ms, mfu 24.59%\n",
      "iter 850: loss 1.3774, balance loss 0.0215 40.33ms, mfu 24.71%\n",
      "iter 860: loss 1.3621, balance loss 0.0213 41.50ms, mfu 24.75%\n",
      "iter 870: loss 1.4260, balance loss 0.0214 41.14ms, mfu 24.81%\n",
      "iter 880: loss 1.3526, balance loss 0.0215 40.60ms, mfu 24.89%\n",
      "iter 890: loss 1.3630, balance loss 0.0214 40.48ms, mfu 24.98%\n",
      "iter 900: loss 1.3472, balance loss 0.0214 40.36ms, mfu 25.06%\n",
      "iter 910: loss 1.3416, balance loss 0.0217 41.21ms, mfu 25.08%\n",
      "iter 920: loss 1.3338, balance loss 0.0217 40.56ms, mfu 25.14%\n",
      "iter 930: loss 1.3683, balance loss 0.0216 39.86ms, mfu 25.24%\n",
      "iter 940: loss 1.3676, balance loss 0.0215 40.22ms, mfu 25.31%\n",
      "iter 950: loss 1.3402, balance loss 0.0215 40.38ms, mfu 25.36%\n",
      "iter 960: loss 1.3631, balance loss 0.0214 40.70ms, mfu 25.38%\n",
      "iter 970: loss 1.3576, balance loss 0.0213 40.11ms, mfu 25.44%\n",
      "iter 980: loss 1.3180, balance loss 0.0215 40.63ms, mfu 25.46%\n",
      "iter 990: loss 1.3141, balance loss 0.0217 40.69ms, mfu 25.48%\n",
      "step 1000: train loss 1.2331, val loss 1.4986\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.3501, balance loss 0.0217 7296.04ms, mfu 22.94%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char_moe.py --compile=False --max_iters=1000 --n_exp=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoE training is slower due to 4x MLP(experts) and additional router. But it gets lower val loss (balance loss is not added during evalutaion for fair comparision)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
